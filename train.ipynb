{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13100663,"sourceType":"datasetVersion","datasetId":8298558}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-18T15:58:03.696955Z","iopub.execute_input":"2025-09-18T15:58:03.697627Z","iopub.status.idle":"2025-09-18T15:58:04.458716Z","shell.execute_reply.started":"2025-09-18T15:58:03.697590Z","shell.execute_reply":"2025-09-18T15:58:04.457948Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/chess-50k/chess_positions.pt\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install chess","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T15:58:48.971159Z","iopub.execute_input":"2025-09-18T15:58:48.971793Z","iopub.status.idle":"2025-09-18T15:58:55.144647Z","shell.execute_reply.started":"2025-09-18T15:58:48.971769Z","shell.execute_reply":"2025-09-18T15:58:55.143889Z"}},"outputs":[{"name":"stdout","text":"Collecting chess\n  Downloading chess-1.11.2.tar.gz (6.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nBuilding wheels for collected packages: chess\n  Building wheel for chess (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for chess: filename=chess-1.11.2-py3-none-any.whl size=147775 sha256=3c589611ff10ca93dab73a27b31928281ee264e855b1b1b6e295fe7823a54d2c\n  Stored in directory: /root/.cache/pip/wheels/fb/5d/5c/59a62d8a695285e59ec9c1f66add6f8a9ac4152499a2be0113\nSuccessfully built chess\nInstalling collected packages: chess\nSuccessfully installed chess-1.11.2\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nimport pytorch_lightning as pl\nfrom pytorch_lightning.loggers import WandbLogger\nfrom pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\nimport wandb\n\nimport math\nimport torch.nn as nn\nfrom typing import Optional\nimport chess\nimport chess.svg\nfrom PIL import Image\nfrom io import BytesIO\nfrom torch.utils.data import Dataset, DataLoader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T15:59:58.099980Z","iopub.execute_input":"2025-09-18T15:59:58.100674Z","iopub.status.idle":"2025-09-18T15:59:58.105129Z","shell.execute_reply.started":"2025-09-18T15:59:58.100647Z","shell.execute_reply":"2025-09-18T15:59:58.104429Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"OUTPUT_FILE = '/kaggle/input/chess-50k/chess_positions.pt'\nABSORBING_STATE_INT = 13\nVOCAB_SIZE = 14  # 12 pieces + 1 empty + 1 absorbing\n\nBATCH_SIZE = 128\nNUM_WORKERS = 4\n\n# Model\nMODEL_DIM = 512\nMODEL_DEPTH = 8\nMODEL_HEADS = 8\n\n# Diffusion\nNUM_TIMESTEPS = 1000\n\n# Training\nLEARNING_RATE = 2e-4\nNUM_EPOCHS = 1\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nPRECISION = \"16-mixed\" if DEVICE == \"cuda\" else \"32\"\n\n# Logging & Sampling\nPROJECT_NAME = \"chess-d3pm\"\nSAMPLE_EVERY_N_STEPS = 500\nNUM_SAMPLES_TO_GENERATE = 4","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T15:59:58.295658Z","iopub.execute_input":"2025-09-18T15:59:58.296209Z","iopub.status.idle":"2025-09-18T15:59:58.301064Z","shell.execute_reply.started":"2025-09-18T15:59:58.296184Z","shell.execute_reply":"2025-09-18T15:59:58.300338Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def tensor_to_fen(board_tensor: torch.Tensor, active_color='w', castling='KQkq', en_passant='-', halfmove_clock=0, fullmove_number=1) -> str:\n    \"\"\"\n    Converts a 1D tensor of 64 tokens back into a FEN string.\n\n    Note: This function only reconstructs the piece placement part of the FEN.\n    Other game state information (turn, castling, etc.) must be provided.\n\n    Args:\n        board_tensor (torch.Tensor): A 1D tensor of shape (64,) representing the board.\n        active_color (str): 'w' or 'b'.\n        castling (str): Castling availability (e.g., 'KQkq', '-', 'Kq').\n        en_passant (str): En passant target square (e.g., 'e3', '-').\n        halfmove_clock (int): Halfmove clock value.\n        fullmove_number (int): Fullmove number.\n\n    Returns:\n        str: The full FEN string for the position.\n    \"\"\"\n    if board_tensor.shape != (64,):\n        raise ValueError(\"Input tensor must have shape (64,)\")\n\n    fen_parts = []\n    for rank_index in range(7, -1, -1):  # Iterate from rank 8 down to 1\n        rank_fen = \"\"\n        empty_count = 0\n        for file_index in range(8):  # Iterate from file 'a' to 'h'\n            square_index = rank_index * 8 + file_index\n            piece_int = board_tensor[square_index].item()\n\n            if piece_int == 0:\n                empty_count += 1\n            else:\n                if empty_count > 0:\n                    rank_fen += str(empty_count)\n                    empty_count = 0\n                rank_fen += INT_TO_PIECE[piece_int]\n\n        if empty_count > 0:\n            rank_fen += str(empty_count)\n\n        fen_parts.append(rank_fen)\n\n    piece_placement = \"/\".join(fen_parts)\n\n    # Combine all parts of the FEN string\n    full_fen = f\"{piece_placement} {active_color} {castling} {en_passant} {halfmove_clock} {fullmove_number}\"\n\n    return full_fen\n\ndef fen_to_tensor(fen_string: str) -> torch.Tensor:\n    \"\"\"\n    Parses a FEN string and converts it into a 1D tensor of 64 tokens.\n    Each token represents a square on the board, with an integer value\n    corresponding to the piece on it (0 for empty).\n\n    Args:\n        fen_string (str): The FEN string for a board position.\n\n    Returns:\n        torch.Tensor: A 1D tensor of shape (64,) with integer piece representations.\n    \"\"\"\n    # The board tensor, initialized to 0 (empty)\n    board_tensor = torch.zeros(64, dtype=torch.long)\n\n    # The first part of FEN is the piece placement\n    piece_placement = fen_string.split(' ')[0]\n\n    rank_index = 7  # Start from rank 8 (index 7)\n    file_index = 0  # Start from file 'a' (index 0)\n\n    for char in piece_placement:\n        if char == '/':\n            rank_index -= 1\n            file_index = 0\n        elif char.isdigit():\n            file_index += int(char)\n        else:\n            square_index = rank_index * 8 + file_index\n            board_tensor[square_index] = PIECE_TO_INT[char]\n            file_index += 1\n\n    return board_tensor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T15:59:58.462581Z","iopub.execute_input":"2025-09-18T15:59:58.463052Z","iopub.status.idle":"2025-09-18T15:59:58.470361Z","shell.execute_reply.started":"2025-09-18T15:59:58.463028Z","shell.execute_reply":"2025-09-18T15:59:58.469736Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def display_board_from_tensor(board_tensor: torch.Tensor, size=400, save_path=None, show_image=False):\n    \"\"\"\n    Generates a visual representation of the board from a tensor.\n\n    Args:\n        board_tensor (torch.Tensor): The 1D tensor of shape (64,) representing the board.\n        size (int): The size of the output image in pixels.\n        save_path (str, optional): Path to save the image file (e.g., 'board.png').\n                                   If None, the image is not saved. Defaults to None.\n        show_image (bool, optional): If True, attempts to open the image in the default\n                                     system viewer. Defaults to False.\n    \"\"\"\n    # Convert the tensor to a FEN string. We use default values for game state\n    # as they don't affect the visual piece placement.\n    fen = tensor_to_fen(board_tensor)\n\n    # Create a board object from the FEN\n    board = chess.Board(fen)\n\n    # Generate an SVG image of the board\n    svg_data = chess.svg.board(board=board, size=size)\n\n    # Convert SVG to a PNG and handle it\n    try:\n        from cairosvg import svg2png\n        png_data = svg2png(bytestring=svg_data.encode('utf-8'))\n        img = Image.open(BytesIO(png_data))\n\n        if save_path:\n            # Ensure the directory exists before saving\n            output_dir = os.path.dirname(save_path)\n            if output_dir:\n                os.makedirs(output_dir, exist_ok=True)\n            img.save(save_path)\n            print(f\"Board image saved to: {save_path}\")\n\n        if show_image:\n            img.show() # This will open the image in your default image viewer\n\n        return img\n    except ImportError:\n        print(\"CairoSVG not found. Cannot display or save image.\")\n        print(\"Board FEN:\", fen)\n        # The SVG data can still be useful for debugging\n        if save_path and save_path.endswith(\".svg\"):\n             with open(save_path, \"w\") as f:\n                f.write(svg_data)\n             print(f\"Saved board as SVG to: {save_path}\")\n        else:\n            print(\"Board SVG data:\\n\", svg_data)\n        return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T15:59:58.606388Z","iopub.execute_input":"2025-09-18T15:59:58.606671Z","iopub.status.idle":"2025-09-18T15:59:58.613070Z","shell.execute_reply.started":"2025-09-18T15:59:58.606653Z","shell.execute_reply":"2025-09-18T15:59:58.612495Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"class ChessDataset(Dataset):\n    \"\"\"\n    A PyTorch Dataset for loading pre-processed chess board tensors.\n    \"\"\"\n    def __init__(self, tensor_file=OUTPUT_FILE):\n        \"\"\"\n        Args:\n            tensor_file (str): Path to the .pt file containing the board tensors.\n        \"\"\"\n        if not os.path.exists(tensor_file):\n            raise FileNotFoundError(\n                f\"Dataset file not found: {tensor_file}. \"\n                f\"Please run create_chess_dataset.py first to generate it.\"\n            )\n        print(f\"Loading dataset from {tensor_file}...\")\n        self.data = torch.load(tensor_file)\n        print(\"Dataset loaded successfully.\")\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T16:00:01.716918Z","iopub.execute_input":"2025-09-18T16:00:01.717188Z","iopub.status.idle":"2025-09-18T16:00:01.722285Z","shell.execute_reply.started":"2025-09-18T16:00:01.717170Z","shell.execute_reply":"2025-09-18T16:00:01.721540Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"class D3PM(nn.Module):\n    \"\"\"\n    The core D3PM engine for discrete state spaces using an absorbing state.\n    \"\"\"\n    def __init__(self, x0_model: nn.Module, n_T: int, num_classes: int, hybrid_loss_coeff=0.0,) -> None:\n        \"\"\"\n        Args:\n            x0_model (nn.Module): The neural network that predicts the original data x_0 from a noisy input x_t.\n            n_T (int): The total number of diffusion timesteps.\n            num_classes (int): The total number of discrete states in the vocabulary, INCLUDING the absorbing state.\n                               For chess, this will be 12 pieces + 1 empty + 1 absorbing = 14.\n            hybrid_loss_coeff (float, optional): The coefficient for the variational bound loss.\n                                                 Defaults to 0.0, which means only CrossEntropyLoss is used.\n        \"\"\"\n        super(D3PM, self).__init__()\n        self.x0_model = x0_model\n        self.n_T = n_T\n        self.num_classes = num_classes\n        self.hybrid_loss_coeff = hybrid_loss_coeff\n        self.eps = 1e-6\n\n        # --- Set up the noise schedule and transition matrices for an absorbing state ---\n\n        # Cosine noise schedule\n        steps = torch.arange(n_T + 1, dtype=torch.float64) / n_T\n        alpha_bar = torch.cos((steps + 0.008) / 1.008 * torch.pi / 2)\n        self.beta_t = torch.minimum(\n            1 - alpha_bar[1:] / alpha_bar[:-1], torch.ones_like(alpha_bar[1:]) * 0.999\n        )\n\n        q_onestep_mats = []\n        # The absorbing state is assumed to be the last class index (num_classes - 1)\n        absorbing_state_idx = self.num_classes - 1\n\n        for beta in self.beta_t:\n            # Create a one-step transition matrix for the absorbing state diffusion process\n            mat = torch.eye(self.num_classes, dtype=torch.float64) * (1 - beta)\n            mat[:, absorbing_state_idx] += beta\n            q_onestep_mats.append(mat)\n\n        q_one_step_mats = torch.stack(q_onestep_mats, dim=0)\n\n        # This will be used for q_posterior_logits\n        q_one_step_transposed = q_one_step_mats.transpose(1, 2)\n\n        # Calculate the cumulative transition matrices q(x_t | x_0) by matrix multiplication\n        q_mat_t = q_one_step_mats[0]\n        q_mats = [q_mat_t]\n        for idx in range(1, self.n_T):\n            q_mat_t = q_mat_t @ q_one_step_mats[idx]\n            q_mats.append(q_mat_t)\n        q_mats = torch.stack(q_mats, dim=0)\n\n        # Register buffers so they are moved to the correct device with the model\n        self.register_buffer(\"q_one_step_transposed\", q_one_step_transposed)\n        self.register_buffer(\"q_mats\", q_mats)\n\n        assert self.q_mats.shape == (self.n_T, self.num_classes, self.num_classes)\n\n    def _at(self, a: torch.Tensor, t: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"Helper function to select rows from a based on t and x.\"\"\"\n        bs = t.shape[0]\n        t_broadcast = t.reshape(bs, *([1] * (x.dim() - 1)))\n        # out[i, j, k, l, m] = a[t[i], x[i, j, k, l], m]\n        return a[t_broadcast - 1, x, :]\n\n    def q_posterior_logits(self, x_0: torch.Tensor, x_t: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Calculates the logits of the posterior distribution q(x_{t-1} | x_t, x_0).\n\n        Args:\n            x_0 (torch.Tensor): The original clean data. Can be integer tensor or logits.\n            x_t (torch.Tensor): The noisy data at timestep t.\n            t (torch.Tensor): The current timestep.\n        \"\"\"\n        # If x_0 is integer, convert it to one-hot logits\n        if x_0.dtype in [torch.int64, torch.int32]:\n            x_0_logits = torch.log(\n                torch.nn.functional.one_hot(x_0, self.num_classes) + self.eps\n            )\n        else:\n            x_0_logits = x_0.clone()\n\n        # Equation (3) from the D3PM paper, simplified for our case\n        fact1 = self._at(self.q_one_step_transposed, t, x_t)\n\n        # We need q_mats for t-2, so handle the t=1 case\n        safe_t = torch.max(t, torch.ones_like(t) * 2)\n        qmats2 = self.q_mats[safe_t - 2].to(dtype=torch.float32)\n\n        softmaxed_x0 = torch.softmax(x_0_logits, dim=-1)\n        fact2 = torch.einsum(\"b...c,bcd->b...d\", softmaxed_x0, qmats2)\n\n        out = torch.log(fact1 + self.eps) + torch.log(fact2 + self.eps)\n\n        # For t=1, the posterior is just the distribution over x_0\n        t_broadcast = t.reshape(t.shape[0], *([1] * (x_t.dim())))\n        return torch.where(t_broadcast == 1, x_0_logits, out)\n\n    def vb(self, dist1: torch.Tensor, dist2: torch.Tensor) -> torch.Tensor:\n        \"\"\"Calculates the KL-divergence for the variational bound loss.\"\"\"\n        dist1_flat = dist1.flatten(start_dim=0, end_dim=-2)\n        dist2_flat = dist2.flatten(start_dim=0, end_dim=-2)\n\n        kl_div = torch.softmax(dist1_flat + self.eps, dim=-1) * (\n            torch.log_softmax(dist1_flat + self.eps, dim=-1)\n            - torch.log_softmax(dist2_flat + self.eps, dim=-1)\n        )\n        return kl_div.sum(dim=-1).mean()\n\n    def q_sample(self, x_0: torch.Tensor, t: torch.Tensor, noise: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        The forward process q(x_t | x_0). Corrupts the clean input x_0 to a noisy x_t.\n        \"\"\"\n        logits = torch.log(self._at(self.q_mats, t, x_0) + self.eps)\n\n        # Use Gumbel-Max trick for sampling from the categorical distribution\n        gumbel_noise = -torch.log(-torch.log(torch.clip(noise, self.eps, 1.0)))\n        return torch.argmax(logits + gumbel_noise, dim=-1)\n\n    def model_predict(self, x_t: torch.Tensor, t: torch.Tensor, cond: Optional[torch.Tensor] = None) -> torch.Tensor:\n        \"\"\"\n        Calls the underlying x0_model to predict the logits of the original data x_0.\n        \"\"\"\n        return self.x0_model(x_t, t, cond)\n\n    def forward(self, x: torch.Tensor, cond: Optional[torch.Tensor] = None) -> tuple[torch.Tensor, dict]:\n        \"\"\"\n        The main training step.\n        1. Samples a timestep t.\n        2. Corrupts the input x to x_t.\n        3. Predicts the original x_0 using the model.\n        4. Calculates the loss.\n        \"\"\"\n        # Sample a random timestep for each item in the batch\n        t = torch.randint(1, self.n_T + 1, (x.shape[0],), device=x.device)\n\n        # Create x_t by running the forward diffusion process\n        noise = torch.rand((*x.shape, self.num_classes), device=x.device)\n        x_t = self.q_sample(x, t, noise)\n\n        # Get the model's prediction of the original x_0's logits\n        predicted_x0_logits = self.model_predict(x_t, t, cond)\n\n        # --- Calculate Loss ---\n        # 1. The primary loss: Cross-Entropy between predicted x_0 and true x_0\n        ce_loss = torch.nn.functional.cross_entropy(\n            predicted_x0_logits.permute(0, 2, 1), # Shape: (B, Vocab, SeqLen)\n            x                                      # Shape: (B, SeqLen)\n        )\n\n        # 2. The auxiliary variational bound loss (optional)\n        vb_loss = torch.tensor(0.0)\n        if self.hybrid_loss_coeff > 0:\n            true_q_posterior = self.q_posterior_logits(x, x_t, t)\n            pred_q_posterior = self.q_posterior_logits(predicted_x0_logits, x_t, t)\n            vb_loss = self.vb(true_q_posterior, pred_q_posterior)\n\n        total_loss = ce_loss + self.hybrid_loss_coeff * vb_loss\n\n        return total_loss, {\n            \"vb_loss\": vb_loss.detach().item(),\n            \"ce_loss\": ce_loss.detach().item(),\n        }\n\n    @torch.no_grad()\n    def p_sample(self, x_t: torch.Tensor, t: torch.Tensor, cond: Optional[torch.Tensor] = None) -> torch.Tensor:\n        \"\"\"\n        The reverse process step p(x_{t-1} | x_t). Samples x_{t-1} given x_t.\n        \"\"\"\n        predicted_x0_logits = self.model_predict(x_t, t, cond)\n        pred_q_posterior_logits = self.q_posterior_logits(predicted_x0_logits, x_t, t)\n\n        # Use Gumbel-Max trick for sampling\n        noise = torch.rand_like(pred_q_posterior_logits)\n        gumbel_noise = -torch.log(-torch.log(torch.clip(noise, self.eps, 1.0)))\n\n        # Don't add noise at the last step (t=1)\n        not_first_step = (t != 1).float().reshape(x_t.shape[0], *([1] * (x_t.dim())))\n        sample = torch.argmax(\n            pred_q_posterior_logits + gumbel_noise * not_first_step, dim=-1\n        )\n        return sample\n\n    @torch.no_grad()\n    def sample(self, initial_noise: torch.Tensor, cond: Optional[torch.Tensor] = None) -> torch.Tensor:\n        \"\"\"\n        Generates a full sample from noise by iterating through the reverse process.\n\n        Args:\n            initial_noise (torch.Tensor): A tensor of shape (B, SeqLen) filled with the absorbing state index.\n            cond (torch.Tensor, optional): Conditioning information, if any. Defaults to None.\n        \"\"\"\n        x = initial_noise\n        for i in reversed(range(1, self.n_T + 1)):\n            t = torch.full((x.shape[0],), i, device=x.device, dtype=torch.long)\n            x = self.p_sample(x, t, cond)\n        return x\n\n    @torch.no_grad()\n    def sample_with_history(self, initial_noise: torch.Tensor, cond: Optional[torch.Tensor] = None, stride: int = 10) -> list[torch.Tensor]:\n        \"\"\"\n        Generates a full sample and saves intermediate steps.\n        \"\"\"\n        x = initial_noise\n        history = []\n        for i in reversed(range(1, self.n_T + 1)):\n            t = torch.full((x.shape[0],), i, device=x.device, dtype=torch.long)\n            x = self.p_sample(x, t, cond)\n            if (i - 1) % stride == 0 or i == 1:\n                history.append(x.cpu())\n        return history\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T16:00:07.476477Z","iopub.execute_input":"2025-09-18T16:00:07.476730Z","iopub.status.idle":"2025-09-18T16:00:07.497846Z","shell.execute_reply.started":"2025-09-18T16:00:07.476713Z","shell.execute_reply":"2025-09-18T16:00:07.496950Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def modulate(x, shift, scale):\n    \"\"\"\n    Modulates the input tensor 'x' using a scale and shift.\n    This is the core of Adaptive Layer Normalization (adaLN).\n    \"\"\"\n    # The unsqueeze is to make the scale and shift broadcastable to the sequence length.\n    return x * (1 + scale.unsqueeze(1)) + shift.unsqueeze(1)\n\nclass TimestepEmbedder(nn.Module):\n    \"\"\"\n    Embeds a discrete timestep t into a continuous vector.\n    \"\"\"\n    def __init__(self, hidden_size, frequency_embedding_size=256):\n        super().__init__()\n        self.mlp = nn.Sequential(\n            nn.Linear(frequency_embedding_size, hidden_size, bias=True),\n            nn.SiLU(),\n            nn.Linear(hidden_size, hidden_size, bias=True),\n        )\n        self.frequency_embedding_size = frequency_embedding_size\n\n    @staticmethod\n    def timestep_embedding(t, dim, max_period=10000):\n        \"\"\"\n        Creates sinusoidal timestep embeddings.\n        Args:\n            t (torch.Tensor): A 1-D Tensor of N indices, one per batch element.\n            dim (int): The dimension of the output.\n            max_period (int): The maximum period for the sinusoidal embeddings.\n        \"\"\"\n        half = dim // 2\n        freqs = torch.exp(\n            -math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half\n        ).to(device=t.device)\n        args = t[:, None].float() * freqs[None]\n        embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n        if dim % 2:\n            embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n        return embedding\n\n    def forward(self, t):\n        t_freq = self.timestep_embedding(t, self.frequency_embedding_size)\n        t_emb = self.mlp(t_freq)\n        return t_emb\n\nclass TransformerBlock(nn.Module):\n    \"\"\"\n    A standard Transformer block with Adaptive Layer Normalization (adaLN).\n    \"\"\"\n    def __init__(self, hidden_size, num_heads, mlp_ratio=4.0, **block_kwargs):\n        super().__init__()\n        self.norm1 = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)\n        self.attn = nn.MultiheadAttention(hidden_size, num_heads, batch_first=True)\n        self.norm2 = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)\n        mlp_hidden_dim = int(hidden_size * mlp_ratio)\n        self.mlp = nn.Sequential(\n            nn.Linear(hidden_size, mlp_hidden_dim),\n            nn.GELU(),\n            nn.Linear(mlp_hidden_dim, hidden_size),\n        )\n        # This single MLP layer generates all the conditioning parameters (scale/shift)\n        # for the entire block. 2 for norm1, 2 for norm2.\n        self.adaLN_modulation = nn.Sequential(\n            nn.SiLU(),\n            nn.Linear(hidden_size, 4 * hidden_size, bias=True)\n        )\n\n    def forward(self, x: torch.Tensor, adaln_input: torch.Tensor) -> torch.Tensor:\n        # Generate scale and shift parameters from the timestep embedding\n        shift_msa, scale_msa, shift_mlp, scale_mlp = self.adaLN_modulation(adaln_input).chunk(4, dim=1)\n\n        # Attention block with adaLN\n        x_norm1 = modulate(self.norm1(x), shift_msa, scale_msa)\n        attn_output, _ = self.attn(x_norm1, x_norm1, x_norm1)\n        x = x + attn_output\n\n        # MLP block with adaLN\n        x_norm2 = modulate(self.norm2(x), shift_mlp, scale_mlp)\n        mlp_output = self.mlp(x_norm2)\n        x = x + mlp_output\n\n        return x\n\nclass FinalLayer(nn.Module):\n    \"\"\"\n    The final layer of the DiT, which projects the sequence of vectors\n    back to the vocabulary space (logits).\n    \"\"\"\n    def __init__(self, hidden_size, out_channels):\n        super().__init__()\n        self.norm_final = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)\n        self.linear = nn.Linear(hidden_size, out_channels, bias=True)\n        self.adaLN_modulation = nn.Sequential(\n            nn.SiLU(),\n            nn.Linear(hidden_size, 2 * hidden_size, bias=True)\n        )\n\n    def forward(self, x, c):\n        shift, scale = self.adaLN_modulation(c).chunk(2, dim=1)\n        x = modulate(self.norm_final(x), shift, scale)\n        x = self.linear(x)\n        return x\n\nclass ChessDiT(nn.Module):\n    \"\"\"\n    A Diffusion Transformer (DiT) specifically for chess board generation.\n    This model predicts the clean board x_0 from a noisy input x_t.\n\n    Args:\n        vocab_size (int): The size of the vocabulary (14 for chess: 12 pieces + empty + absorbing).\n        hidden_size (int): The dimensionality of the model (D).\n        depth (int): The number of Transformer blocks.\n        num_heads (int): The number of attention heads.\n        mlp_ratio (float): The ratio for the MLP's hidden dimension in Transformer blocks.\n    \"\"\"\n    def __init__(\n        self,\n        vocab_size: int,\n        hidden_size: int = 768,\n        depth: int = 12,\n        num_heads: int = 12,\n        mlp_ratio: float = 4.0,\n    ):\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.hidden_size = hidden_size\n        self.num_heads = num_heads\n\n        # 1. Input Embedders\n        self.piece_embedder = nn.Embedding(vocab_size, hidden_size)\n        self.pos_embedder = nn.Parameter(torch.randn(1, 64, hidden_size)) # Learnable positional embeddings\n        self.t_embedder = TimestepEmbedder(hidden_size)\n\n        # 2. Transformer Blocks\n        self.blocks = nn.ModuleList([\n            TransformerBlock(hidden_size, num_heads, mlp_ratio=mlp_ratio) for _ in range(depth)\n        ])\n\n        # 3. Final Output Layer\n        self.final_layer = FinalLayer(hidden_size, vocab_size)\n\n        # Initialize weights\n        self.initialize_weights()\n\n    def initialize_weights(self):\n        # Initialize positional embedding and token embedding\n        nn.init.normal_(self.pos_embedder, std=0.02)\n        nn.init.normal_(self.piece_embedder.weight, std=0.02)\n\n        # Initialize all Linear layers\n        def _basic_init(module):\n            if isinstance(module, nn.Linear):\n                torch.nn.init.xavier_uniform_(module.weight)\n                if module.bias is not None:\n                    nn.init.constant_(module.bias, 0)\n        self.apply(_basic_init)\n\n        # Zero-out output layers:\n        # The last layer of each MLP block\n        for block in self.blocks:\n            nn.init.constant_(block.mlp[-1].bias, 0)\n            nn.init.constant_(block.mlp[-1].weight, 0)\n        # The final projection layer\n        nn.init.constant_(self.final_layer.linear.bias, 0)\n        nn.init.constant_(self.final_layer.linear.weight, 0)\n\n    def forward(self, x_t: torch.Tensor, t: torch.Tensor, cond: Optional[torch.Tensor] = None) -> torch.Tensor:\n        \"\"\"\n        Forward pass of the ChessDiT model.\n\n        Args:\n            x_t (torch.Tensor): The noisy input board tensor of shape (B, 64).\n            t (torch.Tensor): The timestep tensor of shape (B,).\n            cond (Optional[torch.Tensor]): Unused in this unconditional model, kept for API consistency.\n\n        Returns:\n            torch.Tensor: The predicted logits for the clean board x_0, of shape (B, 64, vocab_size).\n        \"\"\"\n        # (B, 64) -> (B, 64, D)\n        x_emb = self.piece_embedder(x_t)\n        # (B,) -> (B, D)\n        t_emb = self.t_embedder(t)\n\n        # Add positional embeddings\n        x = x_emb + self.pos_embedder  # (B, 64, D)\n\n        # Process through Transformer blocks\n        for block in self.blocks:\n            x = block(x, adaln_input=t_emb)\n\n        # Final projection to get logits\n        # The final layer is also conditioned on the timestep\n        logits = self.final_layer(x, t_emb) # (B, 64, D) -> (B, 64, vocab_size)\n\n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T16:00:07.688068Z","iopub.execute_input":"2025-09-18T16:00:07.688341Z","iopub.status.idle":"2025-09-18T16:00:07.705517Z","shell.execute_reply.started":"2025-09-18T16:00:07.688322Z","shell.execute_reply":"2025-09-18T16:00:07.704619Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"class D3PMLightning(pl.LightningModule):\n    \"\"\"\n    PyTorch Lightning wrapper for our D3PM model.\n    This class handles the training, optimization, and logging.\n    \"\"\"\n    def __init__(self, learning_rate: float):\n        super().__init__()\n        self.save_hyperparameters() # Saves learning_rate, etc. to the checkpoint\n\n        # 1. Create the x0-prediction model (the Transformer)\n        x0_model = ChessDiT(\n            vocab_size=VOCAB_SIZE,\n            hidden_size=MODEL_DIM,\n            depth=MODEL_DEPTH,\n            num_heads=MODEL_HEADS,\n        )\n\n        # 2. Create the D3PM diffusion engine\n        self.d3pm = D3PM(\n            x0_model=x0_model,\n            n_T=NUM_TIMESTEPS,\n            num_classes=VOCAB_SIZE,\n            hybrid_loss_coeff=0.0, # Using only CE loss for simplicity\n        )\n\n    def training_step(self, batch, batch_idx):\n        # The batch is a tensor of clean chess boards from our dataset\n        clean_boards = batch\n\n        # The forward pass of our D3PM module does everything:\n        # - picks a random t\n        # - corrupts the input\n        # - runs the model\n        # - calculates the loss\n        loss, info = self.d3pm(clean_boards)\n\n        # Log the loss and its components\n        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n        self.log(\"ce_loss\", info[\"ce_loss\"], on_step=True, logger=True)\n        self.log(\"vb_loss\", info[\"vb_loss\"], on_step=True, logger=True)\n\n        return loss\n\n    def configure_optimizers(self):\n        # The optimizer should only target the parameters of the x0_model\n        optimizer = torch.optim.AdamW(\n            self.d3pm.x0_model.parameters(),\n            lr=self.hparams.learning_rate\n        )\n        return optimizer\n\n    @torch.no_grad()\n    def sample(self, num_samples: int) -> torch.Tensor:\n        \"\"\" Helper function to generate samples from noise. \"\"\"\n        # Start with a board full of the absorbing state token\n        initial_noise = torch.full(\n            (num_samples, 64),\n            ABSORBING_STATE_INT,\n            device=self.device,\n            dtype=torch.long\n        )\n        # Use the D3PM engine's sample method\n        return self.d3pm.sample(initial_noise)\n\nclass SamplingCallback(pl.Callback):\n    \"\"\"\n    A PyTorch Lightning Callback to periodically generate and log board samples.\n    \"\"\"\n    def __init__(self, sample_every_n_steps: int, num_samples: int):\n        super().__init__()\n        self.sample_every_n_steps = sample_every_n_steps\n        self.num_samples = num_samples\n\n    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n        \"\"\" Called after each training step. \"\"\"\n        # Ensure we have a logger and wandb is available\n        if not trainer.logger or not wandb:\n            return\n\n        # Check if it's time to sample\n        global_step = trainer.global_step\n        if (global_step + 1) % self.sample_every_n_steps == 0:\n            print(f\"\\n--- Sampling at step {global_step+1} ---\")\n            pl_module.eval() # Set model to evaluation mode\n\n            # Generate samples\n            generated_boards = pl_module.sample(self.num_samples)\n\n            # Visualize the first generated board\n            first_board_tensor = generated_boards[0].cpu()\n            img = display_board_from_tensor(first_board_tensor, show_image=False)\n\n            if img:\n                # Log the image to wandb\n                trainer.logger.experiment.log({\n                    \"generated_sample\": wandb.Image(img, caption=f\"Step {global_step+1}\")\n                })\n\n            # Also log the FEN string for text-based inspection\n            reconstructed_fen = tensor_to_fen(first_board_tensor)\n            trainer.logger.experiment.log({\n                \"generated_fen\": reconstructed_fen\n            })\n            print(f\"Sampled FEN: {reconstructed_fen}\")\n            print(\"--- End Sampling ---\")\n\n            pl_module.train() # Set model back to training mode","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T16:00:19.476205Z","iopub.execute_input":"2025-09-18T16:00:19.476486Z","iopub.status.idle":"2025-09-18T16:00:19.486617Z","shell.execute_reply.started":"2025-09-18T16:00:19.476467Z","shell.execute_reply":"2025-09-18T16:00:19.485867Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# --- 1. Setup Data ---\nif not os.path.exists(OUTPUT_FILE):\n    raise FileNotFoundError(\n        f\"Dataset file not found: {OUTPUT_FILE}. \"\n        f\"Please run create_chess_dataset.py first to generate it.\"\n    )\ndataset = ChessDataset(tensor_file=OUTPUT_FILE)\ndataloader = DataLoader(\n    dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    num_workers=NUM_WORKERS,\n    pin_memory=True\n)\n\n# --- 2. Setup Model ---\nmodel = D3PMLightning(learning_rate=LEARNING_RATE)\n\n# --- 3. Setup Logging & Callbacks ---\nwandb_logger = WandbLogger(project=PROJECT_NAME, log_model=\"all\")\n# wandb_logger.watch(model, log=\"all\") # Optional: log gradients\n\n# Callback to save the model checkpoint\ncheckpoint_callback = ModelCheckpoint(\n    dirpath=\"checkpoints/\",\n    filename=\"chess-d3pm-{epoch:02d}-{train_loss:.2f}\",\n    save_top_k=3,\n    monitor=\"train_loss\",\n    mode=\"min\",\n)\n\n# Callback for sampling\nsampling_callback = SamplingCallback(\n    sample_every_n_steps=SAMPLE_EVERY_N_STEPS,\n    num_samples=NUM_SAMPLES_TO_GENERATE\n)\n\n# Callback for monitoring learning rate\nlr_monitor = LearningRateMonitor(logging_interval='step')\n\n# --- 4. Setup Trainer and Start Training ---\ntrainer = pl.Trainer(\n    accelerator=DEVICE,\n    precision=PRECISION,\n    max_epochs=NUM_EPOCHS,\n    logger=wandb_logger,\n    callbacks=[checkpoint_callback, sampling_callback, lr_monitor],\n    log_every_n_steps=10,\n)\n\nprint(\"--- Starting Training ---\")\ntrainer.fit(model, dataloader)\nprint(\"--- Training Finished ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T16:00:23.317739Z","iopub.execute_input":"2025-09-18T16:00:23.318235Z"}},"outputs":[{"name":"stdout","text":"Loading dataset from /kaggle/input/chess-50k/chess_positions.pt...\nDataset loaded successfully.\n--- Starting Training ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    "},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}